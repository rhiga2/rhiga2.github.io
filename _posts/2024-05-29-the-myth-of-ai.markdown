---
layout: post
title:  "Book Review: The Myth of Artificial Intelligence"
date:   2024-05-29 16:52:07
categories: book-review
tags: machine-learning book-review
image: /images/myth_of_ai.jpg
published: true
---
**Rating: 2/5**

# Book Review
Erik J. Larson's [*The Myth of Artificial Intelligence: Why Computers Can't Think the Way We Do*][myth_of_ai] challenges the notion that human-like artificial intelligence (sometimes referred to as artificial general intelligence or AGI) is inevitable or even possible to achieve in the next decade. Since the inception of AI in the 1950s, the field has been plagued by distopian (and sometimes utopian prophecie)s where computers are able to make human intelligence rudimentary. The common concern with these visions is that we, as humans, cannot contain superintelligent entity that can outmanuever our measures of control, allowing AI to proliferate in the form of increasingly more intelligent machines. This fear is often referred to as the "singularity", and has only become an increasingly mainstream discussion topic in media and among researchers with the explosion of consumer-facing large language models (LLMs) such as OpenAI ChatGPT, Anthropic Claude, and Google Gemini that ostensibly understand natural language. Larson's central thesis in his book is that these predictions not only lack evidence but are also dangerous to scientific research. Overall, I agreed with the central thesis of Larson's book and found the arguments compelling, but I felt that the book is unfairly critical of big data and machine learning practices that have shown multiple benefits despite being an imcomplete picture of general intelligence. 

# What I Liked
Larson's book is a needed counterbalance to the overwhelming AGI optimism that pervades the field of AI. Ever since AlexNet, a convolutional neural network, smashed records in the ImageNet competition in 2012, researchers, engineers, and even business leaders have been in an arms race to gather more data, create deeper neural networks, and throw computational resources at various benchmarks to achieve highly performant but still narrowly scoped AI. Larson argues that these approaches, as well as massive knowledge stores popular in 1980s-1990s, will not generalize to AGI since they fundamentally ignore abductive reasoning as a key component to intelligence. 

Abductive reasoning is the process of inferring probable causes from observed effects. For example, if the streets are wet you can probably infer that it rained. Throughout the book, Larson argues that abductive reasoning is a missing component not just in the implementation of modern AI but also in the theoretical frameworks that underpin AI research. There are multiple reasons why it's difficult to model abductive reasoning in a theoretical framework:
1. Abduction is often based on single events and not general principles underpinning large numbers of examples, which means that we cannot rely on frequentist approaches popular in big data
2. Abduction requires a lot of pre-existing knowledge about the world that's difficult to encode, organize, and retrieve in a way that's useful for AI even in common sense reasoning tasks. 
3. Abduction can be imprecise since infering causality involves guesswork. We don't know for sure that the streets are wet because it rained, but we can infer that it's very likely. 

One might argue in the past two years since the book was published that LLMs have made great strides towards common sense reasoning needed for abduction as demonstrated by improved performances of AI on benchmark datasets such as HellaSwag and the Winograd Schema Challenge for text and Massive Multi-dicipline Multimodel Understanding (MMMU) for multimodel data. It's still unclear, however, if these models will be able to compose common sense reasoning tasks to form a coherent world model that can be as a ground-truth foundation for abductive inference. In fact, incoherency in world models are a likely reason why LLMs tend to halucinate in practice since they cannot evaluate the truthfulness of statements with a world model that (1) has innumerable knowledge holes and (2) may contain contradictions due to disparate information that is often contained in large scale training sets. Thus while some information regarding common sense reasoning has made significant progress in the past two years, the core elements of Larson's argument as it relates to abductive reasoning hold true for the foreseeable future.         

# What I Disliked
Throughout most of the book, Larson points towards the shortcomings of big data and machine learning practices as it relates to general intelligence, ut towards the second half of the book, Larson directly criticizes the impact of big data and machine learning on scientific research, in particular neuroscience. Larson argues that the rise of big data has led to a decline in the quality of scientific projects and points specifically to the Human Brain Project due to it's overreliance on data, disregard for theory, and general mismanagement. Although Larson's criticisms for the Human Brain Project are valid, the Human Brain Project was plagued by lack of direction, which is not necessarily generalizable to the broader scientific field that has overall benefitted from big data technology. Specifically, Larson's criticism of the project seems to conclude with the notion that scientific theory should preface big data undertakings and not the other way around, but it's also true that data can inform theory and that the two are not mutually exclusive. Having a large bank of data and powerful AI models that can mine insights from data can help researchers form hypothesis and test them in ways that were not possible previously. 

In general, Larson's criticism of big data and machine learning throughout the book tends to appeal to the most egregious examples of hacks that market themselves as general intelligence. For example, he talks extensively about overhyped chatbots such as Eugene Goostman, which portrayed a 13 year old Ukrainian boy with a knack for dodging questions with sarcastic responses, and the heavily hand-engineered, Jeopardy-bot IBM Watson as evidence of AI's failure to navigate general conversations. At the same time, Larson fails to mention the more scientifically-backed systems that existed pre-2022 such as instruct GPT that showed more promising directions for machine natural language understanding. Thus I feel Larson's criticism is not as convincing as it attacks media-hyped AI projects that lacked scientific rigor instead of more sophisticated systems that were avaialble pre-2022.  

# Conclusion
There's a common sentiment that arguing an outcome will happen, whether it be a political shift, an economic recession, or a technological breakthrough is much easier to support than arguing an outcome will not occur. This puts Larson's book at an uphill battle as it's easier to look at the exponential increase in fundings in AI research and appeal to "experts" that benefit from AGI investiment as evidence of the short-term promise of AGI than it is to argue AGI may never come about in our lifetime. Larson's argument as it relates to abductive reasoning still points to knowledge gaps that still need to be solved for AGI to come about. And despite progress in common sense and multimodal inference with the latest LLMs, chaining smaller inferences into long-term reasoning still lacks a theoretical underpinning that would make abduction not only possible but also reliable, reproducible, and generalizable. Although Larson's arguments regarding AGI overhype is convincing, Larson's critcism on the broader field of big data and machine learning as being dangerous for scientific research is not only overexagerated in my opinion, but also unfounded. Appealing the the most eggregious examples of media-hyped , hacky AI systems such as Eugene Goostman happens to make Larson's argument less convincing as it's easier to point out flaws in weak systems compared to more promising, scientific AI research at the time. Overall, I found the book to be a compelling read that provides a needed counterbalance to the media circus around AGI, but I feel it exagerates the dangerous impact of AI and big data on scientific research whilst providing weak and incomplete evidence to support his claims.    

[myth_of_ai]: https://www.amazon.com/Myth-Artificial-Intelligence-Computers-Think-ebook/dp/B08TV31WJ3/ref=sr_1_1?crid=2FS6DY9T5IKVP&dib=eyJ2IjoiMSJ9.vHrRhfX3WRFPLrEF6qmaq564KHbrgmpYbClUOlWw5rsZOgqWF6iDDRY1BtaubHuMQXRv2vLBNK5sesjBURQb5GMvOPakZm1pqxPeDkLfkNNsB6Uzge55Q11j8FDLlTDEyNfc6Olh_2kBOXCkBrAwT_hCmGmGwnHbZrS-Ag9-V4nB5mMQGGxrPNbuk9csW0ChzsGJfLDDpDVzAg6AYajg23YSC49tXFcv_qOxbzEOzmo.OplCg4spgISTYxaucjJfpJSMlj3QBKbBRuQ4faI8Cro&dib_tag=se&keywords=myth+of+artificial+intelligence&qid=1717110367&sprefix=myth+of+artificia%2Caps%2C228&sr=8-1     


